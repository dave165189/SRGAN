{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importing the Necessaary Libraries⚙️**","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/1000/1*-txs0CYANMq5CVi0tp5DCg.png\" width=\"900\" height=\"900\"/>\n\n\n<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*49zf7Kz1URbM7IIcnVd5CQ.png\" width=\"900\" height=\"900\"/>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, math, sys\nimport glob, itertools\nimport argparse, random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.models import vgg19\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image, make_grid\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\nrandom.seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-01T17:12:49.65297Z","iopub.execute_input":"2023-07-01T17:12:49.655374Z","iopub.status.idle":"2023-07-01T17:12:54.725518Z","shell.execute_reply.started":"2023-07-01T17:12:49.655329Z","shell.execute_reply":"2023-07-01T17:12:54.724559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Configuration⚙️**","metadata":{}},{"cell_type":"code","source":"# load pretrained models\nload_pretrained_models = True\n# number of epochs of training\nn_epochs = 3\n# name of the dataset\ndataset_path = \"../input/celeba-dataset/img_align_celeba/img_align_celeba\"\n# size of the batches\nbatch_size = 16\n# adam: learning rate\nlr = 0.00008\n# adam: decay of first order momentum of gradient\nb1 = 0.5\n# adam: decay of second order momentum of gradient\nb2 = 0.999\n# epoch from which to start lr decay\ndecay_epoch = 100\n# number of cpu threads to use during batch generation\nn_cpu = 8\n# high res. image height\nhr_height = 256\n# high res. image width\nhr_width = 256\n# number of image channels\nchannels = 3\nos.makedirs(\"images\", exist_ok=True)\nos.makedirs(\"saved_models\", exist_ok=True)\ncuda = torch.cuda.is_available()\nhr_shape = (hr_height, hr_width)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T17:12:54.727502Z","iopub.execute_input":"2023-07-01T17:12:54.728152Z","iopub.status.idle":"2023-07-01T17:12:54.765105Z","shell.execute_reply.started":"2023-07-01T17:12:54.728119Z","shell.execute_reply":"2023-07-01T17:12:54.764202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Defining Data Class**","metadata":{}},{"cell_type":"code","source":"# Normalization parameters for pre-trained PyTorch models\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\nclass ImageDataset(Dataset):\n    def __init__(self, files, hr_shape):\n        hr_height, hr_width = hr_shape\n        # Transforms for low resolution images and high resolution images\n        self.lr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.hr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.files = files\n    \n    def __getitem__(self, index):\n        img = Image.open(self.files[index % len(self.files)])\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n\n        return {\"lr\": img_lr, \"hr\": img_hr}\n\n    def __len__(self):\n        return len(self.files)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T17:12:54.767271Z","iopub.execute_input":"2023-07-01T17:12:54.767894Z","iopub.status.idle":"2023-07-01T17:12:54.777368Z","shell.execute_reply.started":"2023-07-01T17:12:54.76786Z","shell.execute_reply":"2023-07-01T17:12:54.776368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Get Train/Test Dataloaders♺**","metadata":{}},{"cell_type":"code","source":"train_paths, test_paths = train_test_split(sorted(glob.glob(dataset_path + \"/*.*\")), test_size=0.02, random_state=42)\ntrain_dataloader = DataLoader(ImageDataset(train_paths, hr_shape=hr_shape), batch_size=batch_size, shuffle=True, num_workers=n_cpu)\ntest_dataloader = DataLoader(ImageDataset(test_paths, hr_shape=hr_shape), batch_size=int(batch_size*0.75), shuffle=True, num_workers=n_cpu)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T17:12:54.779836Z","iopub.execute_input":"2023-07-01T17:12:54.780186Z","iopub.status.idle":"2023-07-01T17:12:57.104111Z","shell.execute_reply.started":"2023-07-01T17:12:54.780154Z","shell.execute_reply":"2023-07-01T17:12:57.103127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>Model Architecture</center></h3>\n<img src=\"https://miro.medium.com/max/1000/1*zsiBj3IL4ALeLgsCeQ3lyA.png\" width=\"900\" height=\"900\"/>","metadata":{}},{"cell_type":"markdown","source":"**Generator Architecture:**\n\nThe generator architecture contains residual network instead of deep convolution networks because residual networks are easy to train and allows them to be substantially deeper in order to generate better results. This is because the residual network used a type of connections called skip connections.\nThere are B residual blocks (16), originated by ResNet. Within the residual block, two convolutional layers are used, with small 3×3 kernels and 64 feature maps followed by batch-normalization layers and ParametricReLU as the activation function.\n\nThe resolution of the input image is increased with two trained sub-pixel convolution layers.\n\nThis generator architecture also uses parametric ReLU as an activation function which instead of using a fixed value for a parameter of the rectifier (alpha) like LeakyReLU. It adaptively learns the parameters of rectifier and   improves the accuracy at negligible extra computational cost\n\n During the training, A high-resolution image (HR) is downsampled to a low-resolution image (LR). The generator architecture than tries to upsample the image from low resolution to super-resolution. After then the image is passed into the discriminator, the discriminator and tries to distinguish between a super-resolution and High-Resolution image and generate the adversarial loss which then backpropagated into the generator architecture.","metadata":{}},{"cell_type":"markdown","source":"**Discriminator Architecture:**\n\nThe task of the discriminator is to discriminate between real HR images and generated SR images.   \nThe discriminator architecture used is similar to DC-GAN architecture with LeakyReLU as activation. The network contains eight convolutional layers with of 3×3 filter kernels, increasing by a factor of 2 from 64 to 512 kernels. Strided convolutions are used to reduce the image resolution each time the number of features is doubled. The resulting 512 feature maps are followed by two dense layers and a leakyReLU applied between and a final sigmoid activation function to obtain a probability for sample classification. ","metadata":{}},{"cell_type":"code","source":"class FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n    def forward(self, img):\n        return self.feature_extractor(img)\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n            nn.PReLU(),\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n        )\n\n    def forward(self, x):\n        return x + self.conv_block(x)\nclass GeneratorResNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n        super(GeneratorResNet, self).__init__()\n        # First layer\n        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n        # Residual blocks\n        res_blocks = []\n        for _ in range(n_residual_blocks):\n            res_blocks.append(ResidualBlock(64))\n        self.res_blocks = nn.Sequential(*res_blocks)\n        # Second conv layer post residual blocks\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n        # Upsampling layers\n        upsampling = []\n        for out_features in range(2):\n            upsampling += [\n                # nn.Upsample(scale_factor=2),\n                nn.Conv2d(64, 256, 3, 1, 1),\n                nn.BatchNorm2d(256),\n                nn.PixelShuffle(upscale_factor=2),\n                nn.PReLU(),\n            ]\n        self.upsampling = nn.Sequential(*upsampling)\n        # Final output layer\n        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n\n    def forward(self, x):\n        out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out\nclass Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n        self.model = nn.Sequential(*layers)\n    def forward(self, img):\n        return self.model(img)","metadata":{"execution":{"iopub.status.busy":"2023-07-01T17:12:57.107588Z","iopub.execute_input":"2023-07-01T17:12:57.107875Z","iopub.status.idle":"2023-07-01T17:12:57.13255Z","shell.execute_reply.started":"2023-07-01T17:12:57.10785Z","shell.execute_reply":"2023-07-01T17:12:57.131515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize generator and discriminator\ngenerator = GeneratorResNet()\ndiscriminator = Discriminator(input_shape=(channels, *hr_shape))\nfeature_extractor = FeatureExtractor()\n# Set feature extractor to inference mode\nfeature_extractor.eval()\n# Losses\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_content = torch.nn.L1Loss()\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    feature_extractor = feature_extractor.cuda()\n    criterion_GAN = criterion_GAN.cuda()\n    criterion_content = criterion_content.cuda()\n# Load pretrained models\nif load_pretrained_models:\n    generator.load_state_dict(torch.load(\"/kaggle/input/srgan-saved/generator.pth\"))\n    discriminator.load_state_dict(torch.load(\"/kaggle/input/srgan-saved/discriminator.pth\"))\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\nTensor = torch.cuda.FloatTensor if cuda else torch.Tensor","metadata":{"execution":{"iopub.status.busy":"2023-07-01T17:12:57.134119Z","iopub.execute_input":"2023-07-01T17:12:57.134499Z","iopub.status.idle":"2023-07-01T17:13:09.541623Z","shell.execute_reply.started":"2023-07-01T17:12:57.134466Z","shell.execute_reply":"2023-07-01T17:13:09.540612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gen_losses, train_disc_losses, train_counter = [], [], []\ntest_gen_losses, test_disc_losses = [], []\ntest_counter = [idx*len(train_dataloader.dataset) for idx in range(1, n_epochs+1)]\n\nfor epoch in range(n_epochs):\n    if random.uniform(0, 1) < 0.1:\n        imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n        imgs_hr = make_grid(imgs_hr, nrow=1, normalize=True)\n        gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n        imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n        img_grid = torch.cat((imgs_hr, imgs_lr, gen_hr), -1)\n        save_image(img_grid, f\"images/{batch_idx}.png\", normalize=False)\n\n        # Print previous and new image\n        prev_img = transforms.ToPILImage()(imgs_hr[0])\n        new_img = transforms.ToPILImage()(gen_hr[0])\n        plt.subplot(121)\n        plt.imshow(prev_img)\n        plt.title(\"Previous Image\")\n        plt.subplot(122)\n        plt.imshow(new_img)\n        plt.title(\"New Image\")\n        plt.show()\n\n    ### Training\n    gen_loss, disc_loss = 0, 0\n    tqdm_bar = tqdm(train_dataloader, desc=f'Training Epoch {epoch} ', total=int(len(train_dataloader)))\n    for batch_idx, imgs in enumerate(tqdm_bar):\n        generator.train(); discriminator.train()\n        # Configure model input\n        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        \n        ### Train Generator\n        optimizer_G.zero_grad()\n        # Generate a high resolution image from low resolution input\n        gen_hr = generator(imgs_lr)\n        # Adversarial loss\n        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n        # Content loss\n        gen_features = feature_extractor(gen_hr)\n        real_features = feature_extractor(imgs_hr)\n        loss_content = criterion_content(gen_features, real_features.detach())\n        # Total loss\n        loss_G = loss_content + 1e-3 * loss_GAN\n        loss_G.backward()\n        optimizer_G.step()\n\n        ### Train Discriminator\n        optimizer_D.zero_grad()\n        # Loss of real and fake images\n        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n        # Total loss\n        loss_D = (loss_real + loss_fake) / 2\n        loss_D.backward()\n        optimizer_D.step()\n\n        gen_loss += loss_G.item()\n        train_gen_losses.append(loss_G.item())\n        disc_loss += loss_D.item()\n        train_disc_losses.append(loss_D.item())\n        train_counter.append(batch_idx*batch_size + imgs_lr.size(0) + epoch*len(train_dataloader.dataset))\n        tqdm_bar.set_postfix(gen_loss=gen_loss/(batch_idx+1), disc_loss=disc_loss/(batch_idx+1))\n\n    # Testing\n    gen_loss, disc_loss = 0, 0\n    tqdm_bar = tqdm(test_dataloader, desc=f'Testing Epoch {epoch} ', total=int(len(test_dataloader)))\n    for batch_idx, imgs in enumerate(tqdm_bar):\n        generator.eval(); discriminator.eval()\n        # Configure model input\n        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        \n        ### Eval Generator\n        # Generate a high resolution image from low resolution input\n        gen_hr = generator(imgs_lr)\n        # Adversarial loss\n        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n        # Content loss\n        gen_features = feature_extractor(gen_hr)\n        real_features = feature_extractor(imgs_hr)\n        loss_content = criterion_content(gen_features, real_features.detach())\n        # Total loss\n        loss_G = loss_content + 1e-3 * loss_GAN\n\n        ### Eval Discriminator\n        # Loss of real and fake images\n        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n        # Total loss\n        loss_D = (loss_real + loss_fake) / 2\n\n        gen_loss += loss_G.item()\n        disc_loss += loss_D.item()\n        tqdm_bar.set_postfix(gen_loss=gen_loss/(batch_idx+1), disc_loss=disc_loss/(batch_idx+1))\n        \n        # Save image grid with upsampled inputs and SRGAN outputs\n        if random.uniform(0,1)<0.1:\n            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n            imgs_hr = make_grid(imgs_hr, nrow=1, normalize=True)\n            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n            img_grid = torch.cat((imgs_hr, imgs_lr, gen_hr), -1)\n            save_image(img_grid, f\"images/{batch_idx}.png\", normalize=False)\n\n    test_gen_losses.append(gen_loss/len(test_dataloader))\n    test_disc_losses.append(disc_loss/len(test_dataloader))\n    \n    # Save model checkpoints\n    if np.argmin(test_gen_losses) == len(test_gen_losses)-1:\n        torch.save(generator.state_dict(), \"saved_models/generator.pth\")\n        torch.save(discriminator.state_dict(), \"saved_models/discriminator.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-07-01T17:13:09.543288Z","iopub.execute_input":"2023-07-01T17:13:09.543654Z","iopub.status.idle":"2023-07-02T00:00:04.863002Z","shell.execute_reply.started":"2023-07-01T17:13:09.543621Z","shell.execute_reply":"2023-07-02T00:00:04.859192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_gen_losses, mode='lines', name='Train Generator Loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_gen_losses, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Generator Loss'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Generator Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Adversarial + Content Loss\"),\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T00:01:41.714034Z","iopub.execute_input":"2023-07-02T00:01:41.715061Z","iopub.status.idle":"2023-07-02T00:01:42.602339Z","shell.execute_reply.started":"2023-07-02T00:01:41.715021Z","shell.execute_reply":"2023-07-02T00:01:42.601472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_disc_losses, mode='lines', name='Train Discriminator Loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_disc_losses, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Discriminator Loss'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Discriminator Loss\",\n    xaxis_title=\"Number of training examples seen\",\n    yaxis_title=\"Adversarial Loss\"),\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-02T00:02:00.237236Z","iopub.execute_input":"2023-07-02T00:02:00.237637Z","iopub.status.idle":"2023-07-02T00:02:00.9299Z","shell.execute_reply.started":"2023-07-02T00:02:00.237607Z","shell.execute_reply":"2023-07-02T00:02:00.929119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image as mpimg\ndef plot_images(image1_path, image2_path):\n    # Load the images\n    image1 = mpimg.imread(image1_path)\n    image2 = mpimg.imread(image2_path)\n    # Create a figure and axes\n    fig, axes = plt.subplots(2, 1, figsize=(10,50))\n    # Plot the first image\n    axes[0].imshow(image1)\n    axes[0].axis('off')\n    # Plot the second image\n    axes[1].imshow(image2)\n    axes[1].axis('off')\n    # Adjust the spacing between subplots\n    plt.tight_layout()\n    plt.show()\nimage1_path = '/kaggle/working/images/206.png'\nimage2_path = '/kaggle/working/images/172.png'\nplot_images(image1_path, image2_path)","metadata":{"execution":{"iopub.status.busy":"2023-07-02T00:07:10.750539Z","iopub.execute_input":"2023-07-02T00:07:10.750933Z","iopub.status.idle":"2023-07-02T00:07:13.824965Z","shell.execute_reply.started":"2023-07-02T00:07:10.750899Z","shell.execute_reply":"2023-07-02T00:07:13.81619Z"},"trusted":true},"execution_count":null,"outputs":[]}]}